{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, inspect\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import pymc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from learn_smc_proposals import cde\n",
    "# from learn_smc_proposals.utils import systematic_resample\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.markersize\": 12})\n",
    "sns.set_style('ticks')\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 4\n",
    "n = 1000\n",
    "x = np.linspace(-l, l, n)\n",
    "p = (x>1) * stats.norm(loc=0, scale=1).pdf(x)\n",
    "\n",
    "q = lambda mu, sigma: stats.norm(loc=mu, scale=sigma).pdf(x)\n",
    "res = scipy.optimize.minimize(fun=lambda args: stats.entropy(p, q(args[0], args[1])), x0=[1, 1], method='Nelder-Mead')\n",
    "mu_q, sigma_q = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, p/sum(p*(2*l/n)), label='f(x)p(x|y)')\n",
    "plt.plot(x, q(mu_q, sigma_q), label='q(x)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.show()\n",
    "\n",
    "print(\"mu_q {:.2f}, sigma_q {:.2f}\".format(mu_q, sigma_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 4\n",
    "n = 1000\n",
    "x = Variable(torch.linspace(-l, l, n))\n",
    "p = (x<1).float() * -1e38 + torch.distributions.Normal(0, 1).log_prob(x)\n",
    "\n",
    "mean = Variable(Tensor([0]), requires_grad=True)\n",
    "std = Variable(Tensor([1]), requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([mean, std], lr = 0.01)\n",
    "\n",
    "for t in range(500):\n",
    "    q = torch.distributions.Normal(mean, std).log_prob(x)\n",
    "    loss = torch.dot(p.exp(), p-q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 4\n",
    "n = 1000\n",
    "x = Variable(torch.linspace(-l, l, n)).cuda()\n",
    "p = (x<1).float() * -1e38 + torch.distributions.Normal(0, 1).log_prob(x)\n",
    "\n",
    "mean = Variable(Tensor([0]).cuda(), requires_grad=True)\n",
    "std = Variable(Tensor([1]).cuda(), requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([mean, std], lr = 0.01)\n",
    "\n",
    "for t in range(500):\n",
    "    q = torch.distributions.Normal(mean, std).log_prob(x)\n",
    "    loss = torch.dot(p.exp(), p-q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_q, sigma_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(x) &= \\mathcal{N}(x|\\mu_0, \\sigma_0^2) \\\\\n",
    "p(y|x) &= \\mathcal{N}(y|x, \\sigma^2) \\\\\n",
    "q_\\phi(x|y) &= \\mathcal{N}(x| y; \\phi) \\\\\n",
    "f(x) &= \\mathbb{1}_{x>1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing $$\\mathbb{E}_{p(x,y)}\\left[-f(x) \\log q(x \\mid y;\\phi)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "epochs = 1000\n",
    "H = 10\n",
    "\n",
    "f = lambda x: (x>1).float()\n",
    "\n",
    "q_params = torch.nn.Sequential(\n",
    "          torch.nn.Linear(1, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, 2),\n",
    "        )\n",
    "q_params.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(q_params.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "\n",
    "state_dict = optimizer.state_dict()\n",
    "for param_group in state_dict['param_groups']:\n",
    "    param_group['lr'] = lr\n",
    "optimizer.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "#     x = Variable(torch.distributions.Normal(0, 1).sample((batch_size,)))\n",
    "    x = Variable(torch.distributions.Normal(0, 1).sample_n(batch_size)).cuda()\n",
    "    y = torch.distributions.Normal(x, 1).sample()\n",
    "    \n",
    "#     q_mean, q_log_std = q_params(y.unsqueeze(1)).split(1, dim=1)\n",
    "    q_mean, q_log_std = q_params(y).split(1, dim=1)\n",
    "    q_std = q_log_std.exp()\n",
    "    q = torch.distributions.Normal(q_mean, q_std)\n",
    "    loss = -torch.mean(f(x) * q.log_prob(x))\n",
    "#     print(e, loss.data[0].numpy()) if e%1==0 else None\n",
    "    print(e, loss.data[0]) if e%100==0 else None\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Variable(Tensor([[-1, 0 ,1, 3, -2]])).transpose(0,1)\n",
    "y = Variable(Tensor([[-1.1, 0.1, 1.1, 3.1, -2.1]])).transpose(0,1).cuda()\n",
    "\n",
    "q_mean, q_log_std = q_params(y).split(1, dim=1)\n",
    "q_std = q_log_std.exp()\n",
    "\n",
    "q_mean, q_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May want to substitute fully connected with a NF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try pyro IAFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyro\n",
    "from pyro.distributions.transformed_distribution \\\n",
    "    import InverseAutoregressiveFlow, TransformedDistribution\n",
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = Variable(Tensor([0]), requires_grad=True).cuda()\n",
    "std = Variable(Tensor([1]), requires_grad=True).cuda()\n",
    "\n",
    "base_dist = pyro.distributions.Normal(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iaf = InverseAutoregressiveFlow(input_dim=2, hidden_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = TransformedDistribution(base_dist, [iaf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformedDistribution' object has no attribute 'bijectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b4d910503d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/greyostrich/aims/aims16/agolinsk/miniconda3/envs/torch3gpu/lib/python3.6/site-packages/pyro/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36mlog_pdf\u001b[0;34m(self, y, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mlog_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbijector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mlog_pdf\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbijector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_det_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbijector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformedDistribution' object has no attribute 'bijectors'"
     ]
    }
   ],
   "source": [
    "x = Variable(Tensor([[0]]))\n",
    "dist.log_pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(3,3).tril(-1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
