{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn how to perform regression\n",
    "\n",
    "We're going to train a neural network that knows how to perform inference in robust linear regression models.\n",
    "The network will have as input: \n",
    "\n",
    "* $x$, a vector of input values, and \n",
    "* $y$, a vector of output values.\n",
    "\n",
    "It will learn how to perform posterior inference for the parameter vector $w$, in a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sys, inspect\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "%matplotlib inline\n",
    "import pymc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from learn_smc_proposals import cde\n",
    "from learn_smc_proposals.utils import systematic_resample\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.markersize\": 12})\n",
    "sns.set_style('ticks')\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define 3 binary variables joint distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "# a = a/np.sum(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if not independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal = lambda i: a.sum(axis=tuple({0,1,2}.difference({i})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in itertools.product([0, 1], repeat=3):\n",
    "    assert(not np.isclose(marginal(0)[i[0]]*marginal(1)[i[1]]*marginal(2)[i[2]], a[i[0],i[1],i[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N - number of examples\n",
    "N = 10\n",
    "c = np.digitize(np.random.uniform(size=(N,1)), np.cumsum(a))\n",
    "d = np.unravel_index(c, a.shape)\n",
    "e = np.squeeze(np.dstack(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic(size=100):\n",
    "    c = np.digitize(np.random.uniform(size=(size,1)), np.cumsum(a))\n",
    "    d = np.unravel_index(c, a.shape)\n",
    "    e = np.squeeze(np.dstack(d))\n",
    "    latent = np.atleast_2d(e)\n",
    "    return latent\n",
    "\n",
    "gen_data = lambda num_samples: generate_synthetic(num_samples)\n",
    "example_minibatch = gen_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a network which will invert this model\n",
    "\n",
    "Each of the 3 dimensions of the latent space will be modeled by a mixture of Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalBinaryMADE(\n",
       "  (relu): ReLU()\n",
       "  (layers): ModuleList(\n",
       "    (0): MaskedLinear(in_features=3, out_features=10)\n",
       "    (1): MaskedLinear(in_features=10, out_features=10)\n",
       "  )\n",
       "  (skip_p): MaskedLinear(in_features=3, out_features=3)\n",
       "  (skip_q): MaskedLinear(in_features=3, out_features=3)\n",
       "  (p): MaskedLinear(in_features=10, out_features=3)\n",
       "  (q): MaskedLinear(in_features=10, out_features=3)\n",
       "  (loss): BCELoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_dim = 0\n",
    "\n",
    "latent_dim = 3\n",
    "hidden_units = 10\n",
    "hidden_layers = 2\n",
    "\n",
    "dist_est = cde.ConditionalBinaryMADE(observed_dim, latent_dim, hidden_units, hidden_layers)\n",
    "if torch.cuda.is_available():\n",
    "    dist_est.cuda()\n",
    "\n",
    "dist_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use our network to sample and to compute logpdfs.\n",
    "\n",
    "The primary interface is through `.sample(parents)`, and `.logpdf(parents, latent)`.\n",
    "\n",
    "Both of these expect pytorch tensors as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_minibatch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.cuda.FloatTensor of size 3 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled from p(latent|parents):\n",
      "\n",
      " Variable containing:\n",
      " 0  1  1\n",
      "[torch.cuda.FloatTensor of size 1x3 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.4985  0.5004  0.4971\n",
      "[torch.cuda.FloatTensor of size 1x3 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0  0  0\n",
      "[torch.cuda.FloatTensor of size 1x3 (GPU 0)]\n",
      "\n",
      "3\n",
      "Evaluate log p(latent|parents):\n",
      "\n",
      " Variable containing:\n",
      "-2.0715\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_parents = Variable(torch.FloatTensor())\n",
    "example_latents = Variable(torch.FloatTensor(example_minibatch[0:1]))\n",
    "if torch.cuda.is_available():\n",
    "    example_latents = example_latents.cuda()\n",
    "\n",
    "print(\"Sampled from p(latent|parents):\\n\\n\", dist_est.sample(cuda=True))\n",
    "print(\"Evaluate log p(latent|parents):\\n\\n\", dist_est.logpdf(None, example_latents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize network parameters\n",
    "\n",
    "The `training_epoch` code samples a synthetic dataset, and performs minibatch updates on it for a while. Optionally, it can decide when to stop by examining synthetic validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iterate_minibatches(inputs, outputs, batchsize):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield Variable(torch.FloatTensor(inputs[excerpt])), Variable(torch.FloatTensor(outputs[excerpt]))\n",
    "\n",
    "def training_step(optimizer, dist_est, gen_data, dataset_size, batch_size, max_local_iters=10, misstep_tolerance=0, verbose=False):\n",
    "    \"\"\" Training function for fitting density estimator to simulator output \"\"\"\n",
    "    # Train\n",
    "    synthetic_ins, synthetic_outs = gen_data(dataset_size)\n",
    "    validation_size = dataset_size/10\n",
    "    validation_ins, validation_outs = [Variable(torch.FloatTensor(t)) for t in gen_data(validation_size)]\n",
    "    missteps = 0\n",
    "    num_batches = float(dataset_size)/batch_size\n",
    "    \n",
    "    USE_GPU = dist_est.parameters().__next__().is_cuda\n",
    "    if USE_GPU:\n",
    "        validation_ins = validation_ins.cuda()\n",
    "        validation_outs = validation_outs.cuda()\n",
    "    \n",
    "    validation_err = -torch.mean(dist_est.logpdf(validation_ins, validation_outs)).data[0]\n",
    "    for local_iter in range(max_local_iters):\n",
    "        \n",
    "        train_err = 0 \n",
    "        for inputs, outputs in _iterate_minibatches(synthetic_ins, synthetic_outs, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            if USE_GPU:\n",
    "                loss = -torch.mean(dist_est.logpdf(inputs.cuda(), outputs.cuda()))\n",
    "            else:\n",
    "                loss = -torch.mean(dist_est.logpdf(inputs, outputs))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_err += loss.data[0]/num_batches\n",
    "            \n",
    "        next_validation_err = -torch.mean(dist_est.logpdf(validation_ins, validation_outs)).data[0]\n",
    "        if next_validation_err > validation_err:\n",
    "            missteps += 1\n",
    "        validation_err = next_validation_err\n",
    "        if missteps > misstep_tolerance:\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(train_err, validation_err, \"(\", local_iter+1, \")\")\n",
    "        \n",
    "    return train_err, validation_err, local_iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(dist_est.parameters())\n",
    "trace_train = []\n",
    "trace_validation = []\n",
    "trace_local_iters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_iterations = 1500\n",
    "dataset_size = 2500\n",
    "batch_size = 250\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    verbose = (i+1) % 25 == 0\n",
    "    if verbose:\n",
    "        print(\"[\"+str(1+len(trace_train))+\"]\")\n",
    "    t,v,l = training_step(optimizer, dist_est, gen_data, dataset_size, batch_size, verbose=verbose)\n",
    "    trace_train.append(t)\n",
    "    trace_validation.append(v)\n",
    "    trace_local_iters.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(np.array(trace_train))\n",
    "plt.plot(np.array(trace_validation))\n",
    "plt.legend(['train error', 'validation error']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(trace_local_iters))\n",
    "plt.legend(['iterations per dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting and testing functions\n",
    "\n",
    "We'll use PyMC's default Metropolis-Hastings as a benchmark, and compare to sampling directly from the learned model, and importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_example_pair(model):\n",
    "    model.draw_from_prior()\n",
    "    data_x = model.X.value\n",
    "    data_y = model.y.value\n",
    "    true_w = model.w.value\n",
    "    return data_x, data_y, true_w\n",
    "\n",
    "def estimate_MCMC(data_x, data_y, ns, iters=10000, burn=0.5):\n",
    "    \"\"\" MCMC estimate of weight distribution \"\"\"\n",
    "    mcmc_est = pymc.MCMC(robust_regression(data_x[:,1:2], data_y))\n",
    "    mcmc_est.sample(iters, burn=burn*iters, thin=np.ceil(burn*iters/ns))\n",
    "    trace_w = mcmc_est.trace('w').gettrace()[:ns]\n",
    "    return trace_w\n",
    "\n",
    "def estimate_NN(network, data_x, data_y, ns):\n",
    "    \"\"\" NN proposal density for weights \"\"\"\n",
    "    nn_input = Variable(torch.FloatTensor(np.concatenate((data_x[:,1], data_y[:]))))\n",
    "    print(nn_input.size())\n",
    "    nn_input = nn_input.unsqueeze(0).repeat(ns,1)\n",
    "    if network.parameters().__next__().is_cuda:\n",
    "        nn_input = nn_input.cuda()\n",
    "    values, log_q = network.propose(nn_input)\n",
    "    return values.cpu().data.numpy(), log_q.squeeze().cpu().data.numpy()\n",
    "\n",
    "def sample_prior_proposals(model, ns):\n",
    "    samples = []\n",
    "    for n in range(ns):\n",
    "        model.draw_from_prior()\n",
    "        samples.append(model.w.value)\n",
    "    return np.array(samples)\n",
    "\n",
    "def compare_and_plot(ns=100, alpha=0.05, data_x=None, data_y=None, true_w=None):\n",
    "    model = pymc.Model(robust_regression(None, None))\n",
    "    prior_proposals = sample_prior_proposals(model, ns*10)\n",
    "    if data_x is None:\n",
    "        data_x, data_y, true_w = gen_example_pair(model)\n",
    "    mcmc_trace = estimate_MCMC(data_x, data_y, ns)\n",
    "    nn_proposals, logq = estimate_NN(dist_est, data_x, data_y, ns*10)\n",
    "    mcmc_mean = mcmc_trace.mean(0)\n",
    "    nn_mean = nn_proposals.mean(0)\n",
    "    print()\n",
    "    print(\"True (generating) w:\", true_w)\n",
    "    print(\"MCMC weight mean:\", mcmc_mean)\n",
    "    print(\"NN weight proposal mean:\", nn_mean)\n",
    "    \n",
    "    domain = np.linspace(min(data_x[:,1])-2, max(data_x[:,1])+2, 50)\n",
    "    plt.figure(figsize=(14,3))\n",
    "    plt.subplot(141)\n",
    "\n",
    "    plt.plot(domain, mcmc_mean[0] + mcmc_mean[1]*domain + mcmc_mean[2]*domain**2, \"b--\")\n",
    "    for i in range(ns):\n",
    "        plt.plot(domain, mcmc_trace[i,0] + mcmc_trace[i,1]*domain + mcmc_trace[i,2]*domain**2, \"b-\", alpha=alpha)\n",
    "    plt.plot(data_x[:,1], data_y, \"k.\")\n",
    "    plt.xlim(np.min(domain),np.max(domain))\n",
    "    limy = plt.ylim()\n",
    "    plt.legend([\"MH posterior\"])\n",
    "\n",
    "    ax = plt.subplot(143)\n",
    "    plt.plot(domain, nn_mean[0] + nn_mean[1]*domain + nn_mean[2]*domain**2, \"r--\")\n",
    "    for i in range(ns):\n",
    "        plt.plot(domain, nn_proposals[i,0] + nn_proposals[i,1]*domain  + nn_proposals[i,2]*domain**2, \"r-\", alpha=alpha)\n",
    "    plt.plot(data_x[:,1], data_y, \"k.\")\n",
    "    plt.legend([\"NN proposal\"])\n",
    "    plt.ylim(limy)\n",
    "    plt.xlim(min(domain),max(domain));\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    \n",
    "    ax = plt.subplot(142)\n",
    "    prior_samples_mean = prior_proposals.mean(0)\n",
    "    prior_proposals = prior_proposals[::10]\n",
    "    plt.plot(domain, prior_samples_mean[0] + prior_samples_mean[1]*domain + prior_samples_mean[2]*domain**2, \"c--\")\n",
    "    for i in range(ns):\n",
    "        plt.plot(domain, prior_proposals[i,0] + prior_proposals[i,1]*domain  + prior_proposals[i,2]*domain**2, \"c-\", alpha=alpha)\n",
    "    plt.plot(data_x[:,1], data_y, \"k.\")\n",
    "    plt.legend([\"Prior\"])\n",
    "    plt.ylim(limy)\n",
    "    plt.xlim(min(domain),max(domain));    \n",
    "    ax.yaxis.set_ticklabels([])\n",
    "\n",
    "    # compute NN-IS estimate\n",
    "    logp = []\n",
    "    nn_test_model = pymc.Model(robust_regression(data_x[:,1:2], data_y))\n",
    "    for nnp in nn_proposals:\n",
    "        nn_test_model.w.value = nnp\n",
    "        try:\n",
    "            next_logp = nn_test_model.logp\n",
    "        except:\n",
    "            next_logp = -np.Inf\n",
    "        logp.append(next_logp)\n",
    "    logp = np.array(logp)\n",
    "    w = np.exp(logp - logq) / np.sum(np.exp(logp - logq))\n",
    "    nnis_mean = np.sum(w*nn_proposals.T,1)\n",
    "    print(\"NN-IS estimated mean:\", nnis_mean)\n",
    "    print(\"NN-IS ESS:\", 1.0/np.sum(w**2), w.shape[0])\n",
    "    \n",
    "    ax = plt.subplot(144)\n",
    "    plt.plot(domain, nnis_mean[0] + nnis_mean[1]*domain + nnis_mean[2]*domain**2, \"g--\")\n",
    "    \n",
    "    nn_resampled = nn_proposals[systematic_resample(np.log(w))][::10]\n",
    "    for i in range(ns):\n",
    "        plt.plot(domain, nn_resampled[i,0] + nn_resampled[i,1]*domain  + nn_resampled[i,2]*domain**2, \"g-\", alpha=alpha)\n",
    "    plt.plot(data_x[:,1], data_y, \"k.\")\n",
    "    plt.legend([\"NN-IS posterior\"])\n",
    "    plt.ylim(limy)\n",
    "    plt.xlim(min(domain),max(domain));\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_and_plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
